{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0000001",
   "metadata": {},
   "source": [
    "# Makemore Part 1: Bigram Character-Level Language Model\n",
    "\n",
    "Building a character-level language model that generates name-like words. Two approaches:\n",
    "1. **Counting-based**: Directly estimate bigram probabilities from data\n",
    "2. **Neural network**: Learn the same probabilities via gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000002",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14838230-a929-44d5-8985-195c8697946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0733f0-e94e-4b7c-9c6a-203525897899",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()\n",
    "print(f'{len(words)} names, lengths {min(len(w) for w in words)}-{max(len(w) for w in words)}')\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000003",
   "metadata": {},
   "source": [
    "## Character Vocabulary\n",
    "\n",
    "Map each character to an integer index. The special token `.` (index 0) represents both start and end of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be38e70-da50-4c24-8bd0-579746e8e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000004",
   "metadata": {},
   "source": [
    "## Approach 1: Bigram Counting\n",
    "\n",
    "Count how often each character follows another across all names, then normalize to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c876d-3c42-4706-943d-7f2d4fb5567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        N[idx1, idx2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000005",
   "metadata": {},
   "source": [
    "### Sampling from the Model\n",
    "\n",
    "Add-one smoothing prevents zero probabilities for unseen bigrams. Then sample autoregressively until the end token `.` is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fb82e-3e6d-44f1-ac0c-5599918b9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P = P / P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a732d-c321-40dd-af41-4b152c3e18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(252525)\n",
    "\n",
    "for i in range(15):\n",
    "    out = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        p = P[idx]\n",
    "        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[idx])\n",
    "        if idx == 0:\n",
    "            print(''.join(out))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000006",
   "metadata": {},
   "source": [
    "### Evaluating with Negative Log Likelihood\n",
    "\n",
    "The quality metric: average negative log likelihood over the dataset. Lower is better. A perfect model that memorized the training set would have NLL = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb42c78-ebfe-49c0-afd7-f7b77adda406",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_like = 0.0\n",
    "count = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        p = P[idx1, idx2]\n",
    "        log_like += torch.log(p)\n",
    "        count += 1\n",
    "\n",
    "neg_ll = -log_like\n",
    "norm_nll = neg_ll / count\n",
    "print(f'Neg Logll = {neg_ll:.4f} | Norm Neg Logll = {norm_nll:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000007",
   "metadata": {},
   "source": [
    "## Approach 2: Neural Network (Gradient-Based)\n",
    "\n",
    "Learn the same bigram probabilities using a single-layer neural network trained with gradient descent. The network takes a one-hot encoded character and outputs a probability distribution over the next character via softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000008",
   "metadata": {},
   "source": [
    "### Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ee8ab-3e25-42ac-bc20-28fa47becb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f'{len(xs)} bigram examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000009",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Each input character is represented as a 27-dimensional one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7aed4c-db78-4ef9-9aa6-ef93e997e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs[:5], num_classes=27).float()\n",
    "plt.imshow(xenc)\n",
    "plt.title('One-hot encoding of first 5 bigram inputs')\n",
    "plt.xlabel('Character index')\n",
    "plt.ylabel('Example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000010",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "The forward pass: one-hot input x weight matrix -> logits -> exponentiate -> normalize (softmax). This is equivalent to learning a 27x27 probability table, the same structure as the counting approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aead1-68a4-4325-9d0e-abc234230ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93695460-3200-4363-8d6d-8cfaf3a7e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "print(f'Initial loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000011",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74686845-843b-4d70-b2ed-fa1367661901",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "for i in range(100):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -100 * W.grad\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(f'Step {i:3d} | Loss = {loss.item():.4f}')\n",
    "\n",
    "print(f'Final loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}